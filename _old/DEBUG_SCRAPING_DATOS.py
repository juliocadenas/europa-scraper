#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SCRIPT DE DIAGN√ìSTICO COMPLETO DEL PROBLEMA DE EXTRACCI√ìN DE DATOS
=================================================================

Este script va a diagnosticar POR QU√â los archivos CSV no tienen datos reales
y va a identificar exactamente d√≥nde est√° el problema en el flujo de scraping.
"""

import asyncio
import logging
import sys
import os
from datetime import datetime

# Configurar logging detallado
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'diagnostico_scraping_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# A√±adir directorio ra√≠z al path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

async def diagnosticar_buscador_duckduckgo():
    """Diagnosticar espec√≠ficamente el buscador DuckDuckGo"""
    logger.info("üîç DIAGN√ìSTICO ESPEC√çFICO: DuckDuckGo")
    
    try:
        from utils.scraper.browser_manager import BrowserManager
        from utils.scraper.text_processor import TextProcessor
        from utils.scraper.search_engine import SearchEngine
        
        # Inicializar componentes
        browser_manager = BrowserManager()
        text_processor = TextProcessor()
        search_engine = SearchEngine(browser_manager, text_processor)
        
        # Verificar navegador
        logger.info("üåê Verificando navegador Playwright...")
        browser_ok = await browser_manager.check_playwright_browser()
        logger.info(f"   Estado del navegador: {'‚úÖ OK' if browser_ok else '‚ùå ERROR'}")
        
        if not browser_ok:
            logger.error("‚ùå Navegador no disponible - ESTE ES EL PROBLEMA")
            return False
        
        # Probar b√∫squeda simple
        logger.info("üîç Probando b√∫squeda simple en DuckDuckGo...")
        test_query = "agricultural production crops"
        
        try:
            results = await search_engine.search_duckduckgo(test_query)
            logger.info(f"   Resultados obtenidos: {len(results)}")
            
            if results:
                logger.info("‚úÖ B√∫squeda DuckDuckGo funcionando:")
                for i, result in enumerate(results[:3]):
                    logger.info(f"   Resultado {i+1}: {result.get('title', 'No title')}")
                    logger.info(f"   URL: {result.get('url', 'No URL')}")
                    logger.info(f"   Descripci√≥n: {result.get('description', 'No desc')[:100]}...")
                return True
            else:
                logger.error("‚ùå B√∫squeda DuckDuckGo no devolvi√≥ resultados")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error en b√∫squeda DuckDuckGo: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error general en diagn√≥stico DuckDuckGo: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

async def diagnosticar_extraccion_contenido():
    """Diagnosticar la extracci√≥n de contenido de URLs"""
    logger.info("üìÑ DIAGN√ìSTICO DE EXTRACCI√ìN DE CONTENIDO")
    
    try:
        from utils.scraper.content_extractor import ContentExtractor
        from utils.scraper.browser_manager import BrowserManager
        
        browser_manager = BrowserManager()
        content_extractor = ContentExtractor(browser_manager)
        
        # URL de prueba
        test_url = "https://example.com"
        logger.info(f"üîç Probando extracci√≥n de contenido de: {test_url}")
        
        try:
            content = await content_extractor.extract_content(test_url)
            logger.info(f"   T√≠tulo extra√≠do: {content.get('title', 'No title')}")
            logger.info(f"   Longitud del texto: {len(content.get('text', ''))}")
            logger.info(f"   Palabras totales: {content.get('word_count', 0)}")
            
            if content.get('text'):
                logger.info("‚úÖ Extracci√≥n de contenido funcionando")
                return True
            else:
                logger.error("‚ùå No se pudo extraer texto del contenido")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error en extracci√≥n de contenido: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error general en diagn√≥stico de extracci√≥n: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

async def diagnosticar_workflow_completo():
    """Diagnosticar el workflow completo de scraping"""
    logger.info("üîÑ DIAGN√ìSTICO DEL WORKFLOW COMPLETO")
    
    try:
        from utils.scraper.controller import ScraperController
        
        # Crear controller de prueba
        controller = ScraperController()
        
        # Datos de prueba
        test_data = {
            'sic_code': '01.0',
            'course_name': 'AGRICULTURAL PRODUCTION CROPS',
            'search_engine': 'DuckDuckGo',
            'max_results': 5
        }
        
        logger.info(f"üîç Probando workflow completo con: {test_data}")
        
        try:
            results = await controller.scrape_course(test_data)
            logger.info(f"   Resultados del workflow: {len(results)}")
            
            if results:
                logger.info("‚úÖ Workflow completo funcionando:")
                for i, result in enumerate(results[:3]):
                    logger.info(f"   Resultado {i+1}:")
                    for key, value in result.items():
                        logger.info(f"     {key}: {str(value)[:100]}...")
                return True
            else:
                logger.error("‚ùå Workflow completo no devolvi√≥ resultados")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error en workflow completo: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error general en diagn√≥stico workflow: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def diagnosticar_archivos_resultados():
    """Diagnosticar archivos de resultados existentes"""
    logger.info("üìÅ DIAGN√ìSTICO DE ARCHIVOS DE RESULTADOS")
    
    import glob
    import csv
    
    # Buscar archivos CSV recientes
    csv_files = glob.glob("results/*.csv")
    logger.info(f"   Archivos CSV encontrados: {len(csv_files)}")
    
    for csv_file in csv_files[-5:]:  # √öltimos 5 archivos
        logger.info(f"   Analizando: {csv_file}")
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                rows = list(reader)
                logger.info(f"     Filas totales: {len(rows)}")
                
                if rows:
                    sample_row = rows[0]
                    logger.info("     Columnas:")
                    for col in sample_row.keys():
                        value = sample_row.get(col, '')
                        logger.info(f"       {col}: {str(value)[:50]}...")
                        
                    # Verificar si los datos son de prueba
                    if 'example.com' in str(sample_row.get('url', '')):
                        logger.warning("     ‚ö†Ô∏è  DATOS DE PRUEBA DETECTADOS")
                    elif len(sample_row.get('description', '')) < 50:
                        logger.warning("     ‚ö†Ô∏è  DESCRIPCI√ìN MUY CORTA - POSIBLE ERROR")
                    else:
                        logger.info("     ‚úÖ DATOS PARECEN REALES")
                else:
                    logger.warning("     ‚ö†Ô∏è  ARCHIVO SIN DATOS")
                    
        except Exception as e:
            logger.error(f"     ‚ùå Error leyendo archivo: {str(e)}")

async def main():
    """Funci√≥n principal de diagn√≥stico"""
    logger.info("üöÄ INICIANDO DIAGN√ìSTICO COMPLETO DEL PROBLEMA DE SCRAPING")
    logger.info("="*80)
    
    # Diagn√≥stico 1: Buscador DuckDuckGo
    duckduckgo_ok = await diagnosticar_buscador_duckduckgo()
    logger.info("="*80)
    
    # Diagn√≥stico 2: Extracci√≥n de contenido
    contenido_ok = await diagnosticar_extraccion_contenido()
    logger.info("="*80)
    
    # Diagn√≥stico 3: Workflow completo
    workflow_ok = await diagnosticar_workflow_completo()
    logger.info("="*80)
    
    # Diagn√≥stico 4: Archivos de resultados
    diagnosticar_archivos_resultados()
    logger.info("="*80)
    
    # Resumen del diagn√≥stico
    logger.info("üìã RESUMEN DEL DIAGN√ìSTICO:")
    logger.info(f"   DuckDuckGo: {'‚úÖ OK' if duckduckgo_ok else '‚ùå ERROR'}")
    logger.info(f"   Extracci√≥n contenido: {'‚úÖ OK' if contenido_ok else '‚ùå ERROR'}")
    logger.info(f"   Workflow completo: {'‚úÖ OK' if workflow_ok else '‚ùå ERROR'}")
    
    if not duckduckgo_ok:
        logger.error("üî• PROBLEMA IDENTIFICADO: El buscador DuckDuckGo no est√° funcionando")
        logger.error("   Soluci√≥n: Revisar configuraci√≥n del navegador y conexi√≥n")
    elif not contenido_ok:
        logger.error("üî• PROBLEMA IDENTIFICADO: La extracci√≥n de contenido no est√° funcionando")
        logger.error("   Soluci√≥n: Revisar ContentExtractor y procesamiento de HTML")
    elif not workflow_ok:
        logger.error("üî• PROBLEMA IDENTIFICADO: El workflow completo no est√° funcionando")
        logger.error("   Soluci√≥n: Revisar ScraperController y flujo de procesamiento")
    else:
        logger.info("‚úÖ TODOS LOS COMPONENTES EST√ÅN FUNCIONANDO")
        logger.info("   El problema podr√≠a estar en la configuraci√≥n o en los datos de entrada")

if __name__ == "__main__":
    asyncio.run(main())